---
title: "pgc-perf-opt Pipeline"
subtitle: "A SARS-CoV-2 Variant Discriminator using Unsupervised Machine Learning (UML) Techniques"
author: "<b>Project Lead:</b> Mark Joseph Garrovillas, RCh</br><b>Interns:</b> Brylle Joshua Logro√±o, John Matthew Felices, Yenzy Urson Hebron"
date: "July 2023"
institution: "Philippine Genome Center - Core Facility for Bioinformatics"
output: html_notebook
---

This notebook tackles the main code in `code/pipeline.R`, guiding us through the workflow's setup and execution, along with a brief exploration of the results. Code chunks that are not present or referred to in the main code, such as the setup chunk, are excluded from previews and knits.

This workflow can produce rich and intuitive clusterings of SARS-CoV-2 genome samples by treating $k$-mer count data with the following dimensionality reduction and clustering techniques:

* Principal components analysis (PCA)
* t-distributed stochastic neighbor embedding (t-SNE)
* Uniform manifold approximation (UMAP), and
* Agglomerative clustering (AGNES).

The efficiency and quality of the clusterings are also assessed, particularly the insights that can be extracted from the resulting visualizations. The $k$-mer counts of each sample are also augmented with metadata, which while not used to *perform* the clusterings to preserve the UML approach, enhances the results with descriptive labels.

The workflow is well-tested on odd $k$-mer count matrices where $k = \{3, 5, 7\}$. It is currently set to only use Philippine SARS-CoV-2 data (see `data/GISAID`), filtering out samples not collected in the Philippines (see `country_exposure` parameter), but it can be set to use other countries in the parameters section of the Setup.

# 1. Setup
```{r setup, echo=FALSE}
# Executing this setup chunk will set the working directory to that of the
# project repository root and will add "Copy to Clipboard" buttons in the code
# chunks via `klippy`.
knitr::opts_knit$set(root.dir = normalizePath(".."))
if (!require("klippy"))
  devtools::install_github("rlesur/klippy", upgrade = FALSE)
library(klippy)
```

```{r klippy, echo=FALSE, include=FALSE}
  klippy::klippy()
```

## A. Load Packages
If this project is not running in a project-specific environment (e.g. the custom-made pgc-perf-opt Docker image), then the following packages will also have to be installed for the pipeline to work. Else, this section will only deal with loading the dependencies.

We use the CRAN repository as our main source.
```{r message=FALSE}
options(repos = "https://cloud.r-project.org/")
```

We use the `pacman` package to conveniently organize our project's packages.
```{r message=FALSE}
if (!require("pacman"))
  install.packages("pacman")
library(pacman)
```

If we are not running on a tailor-made environment, we also install `xml2` in advance to prepare for Tidyverse installation in Linux. Note that in Windows RStudio, this is installed by default. This is also a good time to demonstrate how our development life cycle took care to maintain compatibility between Linux and Windows, and this is most evident in the pipeline and in `code/R/helper.R::write_to_log`.
```{r message=FALSE}
if (pacman::p_detectOS() == "Linux" && !pacman::p_exists(xml2, local = TRUE)) {
  install.packages("xml2", dependencies = TRUE, INSTALL_opts = c("--no-lock"))
  pacman::p_load(xml2)
}
```

Let us use `pacman` to load add-on packages as desired. We omit redundant dependencies such as `dplyr` and `ggplot2` which are already in the `tidyverse`.
```{r message=FALSE}
pacman::p_load(plyr, GGally, ggthemes, ggvis, plotly, psych,
               htmlwidgets, rio, markdown, shiny, tidyverse,
               ape, seqinr, kmer, validate, gsubfn,
               Rtsne, tsne, umap, factoextra, scales,
               RColorBrewer, ggfortify, devtools,
               ggdendro, dendextend, cluster, colorspace,
               microbenchmark, data.table, highcharter)

# ggbiplot is installed separately via install_github
if (!require(ggbiplot))
  devtools::install_github("vqv/ggbiplot", upgrade = FALSE, quiet = TRUE)
pacman::p_load(ggbiplot)
```

Remarks on some of the dependencies:

* `ape`: for reading and writing FASTA files in `DNAbin` format.
* `seqinr`: an `ape` alternative, reads and writes FASTA files in `char` format.
* `kmer`: for generating $k$-mer counts, works more efficiently with `DNAbin`.
* `validate`: for the `%vin%` membership operator.
* `gsubfn`: for destructuring more than one return value.
* `Rtsne`, `tsne`, `umap`, `ggdendro`, and `dendextend`: for accessing t-SNE, UMAP, and AGNES algorithms.
* `devtools`: supports `install_github` for installing `ggbiplot`.

## B. Load Sources
Let us fetch functions from the files in `code/R`. This requires that this notebook is loaded using the pgc-perf-opt `.Rproject` file at the root of the repository.
```{r}
source("code/R/helper.R")
source("code/R/preprocess.R")
source("code/R/kmer-analysis.R")
source("code/R/dim-reduce.R")
source("code/R/clustering.R")
```

## C. Set Parameters
Here one may set parameters that will control the flow and behavior of the pipeline.

### Pipeline General Parameters
Pipeline general parameters are shared by multiple routines.

* `seed` (`int`): the value used to reproduce the randomization of stochastic routines like `sample_n` and `tsne`. We default to `1234`.
* `stamp` (`str|NULL`): the suffix added to files generated by the preprocess subroutines.
  + These generated files can be found in `data/interm`, `data/overview`, and `data/kmers`.
  + Any stamp is valid, but the recommended stamps are "`str(strat_size)`" (same string as `strat_size` value, to be discussed later), "`get_time()`", and either `NULL` or "" (empty string) for no stamp.
* `write_fastacsv` (`bool`): controls writing of intermediate files.
* `kmer_list` (`c(int)`): list of $k$ values to evaluate.
  + Default in `pipeline.R` is `c(3, 5, 7)` but not in this notebook, where only $k=7$ will be the focus of evaluations.
* `strat_size` (`int`): number of samples randomly selected per stratum.
  + Current `nrow(data) = 24761`, hence `strat_size <- 25000` will ensure the selection of ALL samples in the entire data.
  + Valid `strat_size` will only be those with corresponding files in both `data/interm` and `data/kmers`, e.g. if there are `data/interm/`.
  + This parameter only serves to limit the amount of samples to process for low-performance computers.
* `include_plots` (`bool`): whether or not to generate plots.
  + Used by `dim-reduce.R` and `clustering.R` (AGNES) routines.

```{r}
seed <- 1234
stamp <- get_time()
write_fastacsv <- TRUE
kmer_list <- c(7)
strat_size <- 100
include_plots <- TRUE
```

### preprocess.R Parameters
These parameters control how the raw GISAID data is processed from extraction to preparing the FASTA and metadata files for downstream analysis. The variable names are self-explanatory, aside from `country_exposure` which dictates the country to filter for in `preprocess.R::get_sample()`.

```{r}
# where to read GISAID data and where to extract it
gisaid_data_path <- "data/GISAID"
gisaid_extract_path <- "data/GISAID/datasets"

# filter GISAID data by which country
country_exposure <- "Philippines"

# where to write intermediate data (fasta_all and metadata_all)
interm_write_path <- "data/interm"

# where to write overviews, including treemaps and heatmaps
compile_write_path <- "data/overview"
```

### kmer-analysis.R Parameters
There are no parameters for `kmer-analysis.R` routines. Said routines will always take FASTA and metadata passed from the global environment, hence running `preprocess.R` routines to completion at least once is necessary before running `kmer-analysis.R` routines. `kmer-analysis.R` will also always write its output to `data/kmers`.

### dim-reduce.R Parameters
These parameters control the dimensionality reduction algorithms. Hypertuning these parameters may be in order to achieve good and efficient clustering. Please consult `code/R/dim-reduce.Rmd` for more in-depth explanation on `dim-reduce.R`. Also see `?tsne::tsne` and `?umap::umap` for more information. Brief explanations of these parameters now follow:

* `kmers_data_path` (`str`): where to fetch the kmer data to be processed.
* `dimreduce_write_path` (`str`): where to write results of dimensionality reduction.
* `tsne_perplexity` (`int`): guess about the number of close neighbors each point has.
* `tsne_max_iter` (`int`): bounds the number of `tsne` iterations
  + Limited to 1000 to avoid infinite convergence, but may be increased in faster computers.
* `tsne_initial_dims` (`int`): number of dimensions to "start" from, more aggressively collapsing higher dimensions.
* `umap_n_neighbors` (`int`):  constrains the size of the local neighborhood where the manifold structure of the data may be learned.
* `umap_metric` (`str`): determines how distances between data points are computed.
  + Available metrics are: euclidean, manhattan, cosine, pearson, and pearson2.
* `umap_min_dist` (`int`): determines how close points appear in the final layout.
* `color`: determines which metadata attribute to use for assigning colors to the clusters.
  + See metadata columns for available values.
* `shape`: determines which metadata attribute to use for assigning shapes to the clusters.
  + See metadata columns for available values.
  
Note that `color` and `shape` work together to determine a two-level clustering for the dimensionality reduction visualizations.

```{r}
kmers_data_path <- "data/kmers"
dimreduce_write_path <- "results/dim-reduce/R"
tsne_perplexity <- 40
tsne_max_iter <- 1000
tsne_initial_dims <- 50
umap_n_neighbors <- 15
umap_metric <- "euclidean"
umap_min_dist <- 0.1
color <- "variant"
shape <- "year"

# dim-reduce.R Filters - OPTIONAL
# factor1 <- "variant"
# values1 <- c("Omicron", "Omicron Sub")
# factor2 <- "year"
# values2 <- c("2023")
```

### clustering.R Parameters
```{r}
agnes_write_path <- "results/dendrogram"
```

# 2. Run Pipeline
Instead of functions sourced externally (as we did in the setup), they are expanded here for better understanding and coherence. We still show at the top how the routine call originally looked like in the pipeline.

## A. get_sample()
This function is responsible for 

**Sourced call:**
```{r}
get_sample <- function(gisaid_data_path = "data/GISAID",
                       gisaid_extract_path = "data/GISAID/datasets",
                       seed = 1234, strat_size = 100,
                       country_exposure = "Philippines") {
  # Extract GISAID data.
  if (dir.exists(gisaid_extract_path)) {
    message("GISAID data already extracted from tar archives.")
  } else {
    message("Extracting GISAID data to data/GISAID/datasets/...")
    tars <- list.files(gisaid_data_path, pattern = ".+\\.tar")
    for (file_name in tars) {
      subdir <- str_match(file_name, pattern = "[^-]+-[^-]+")
      message(paste("Extracting to:", subdir))
      untar(paste(gisaid_data_path, file_name, sep = "/"),
            exdir = paste(gisaid_extract_path, subdir, sep = "/"))
    }
  }
  
  # Merge all extracted fasta and tsv files.
  ## fasta contains sequence, while tsv contains metadata.
  omicron_sub = c("ba275", "xbb", "xbb_1.5", "xbb_1.16", "xbb1.91")
  
  fastas <- list.files(gisaid_extract_path, recursive = TRUE,
                       pattern = ".+\\.fasta")
  tsvs <- list.files(gisaid_extract_path, recursive = TRUE,
                     pattern = ".+\\.tsv")
  nfiles <- length(fastas)
  
  ## Accumulators: fasta_all and metadata_all
  fasta_all <- list()
  metadata_all <- tibble()

  ## Warnings suppressed for data parsing, but handled cleanly so don't worry.
  suppressWarnings({
    for (i in 1:nfiles) {
      fasta_path <- paste(gisaid_extract_path, fastas[i], sep = "/")
      tsv_path <- paste(gisaid_extract_path, tsvs[i], sep = "/")
      variant <- str_match(fasta_path, pattern = "(?<=-).*(?=\\/)")
      if (variant %vin% omicron_sub) {
        variant <- "Omicron Sub"
      }
      variant <- str_to_title(variant)
      
      message(paste0("Reading ", fasta_path, "... "))
      # Parse then merge fasta file with accumulator.
      fasta <- ape::read.FASTA(fasta_path)
      fasta_all <- append(fasta_all, fasta)
      message("\bDONE.")
      
      message(paste0("Reading ", tsv_path, "... "))
      # Parse then merge metaData file with accumulator.
      # Defer sanitation after random sampling so fasta and metaData kept 1:1.
      # Can't directly col_types = "c_c_D____ccc___if_c__cc_____" because
      # of dirt in some columns, still need to use characters.
      metaData <- readr::read_tsv(tsv_path,
                                  col_select = c(1,3,5,10,11,12,16,17,19,22,23),
                                  show_col_types = FALSE)
      message("\bDONE.")
      
      # Not removing raw date as I believe it is useful for sorting or can be
      # parsed on an as-needed basis. Dropped year, month, day: just extract
      # them from the date using lubridate::{year,month,day}(date).
      metaData <- metaData %>%
        dplyr::mutate(variant = as.character(variant))
      
      # Note: Cannot use tidyr::nest(fasta or tibble(fasta)), see reason below.
      
      # Coerce guessed column types to correct types (also for bind_rows).
      # NAs introduced by coercion will be dropped later because dropping
      # metadata rows must be consistent with dropping fasta entries for
      # the reason that nested DNAbin lists are not supported in R.
      metaData$age <- as.integer(metaData$age)
      metaData$sex <- as.character(metaData$sex)
      
      metadata_all <- bind_rows(metadata_all, metaData)
    }
  })

  rm(fasta)
  rm(metaData)

  # Print out number of samples beforehand to guide future strat_size.
  message(paste("\nTotal number of samples in complete, unpruned data:",
              nrow(metadata_all)))
  message("Variant distribution in complete data:")
  metadata_all %>%
    dplyr::group_by(variant) %>%
    dplyr::count() %>% print()

  # Addon: Filter by country_exposure.
  drop_idxs <- which(metadata_all$country_exposure != country_exposure)
  fasta_all <- fasta_all[is.na(pmatch(1:length(fasta_all), drop_idxs))]
  metadata_all <- metadata_all[is.na(pmatch(1:nrow(metadata_all), drop_idxs)),]
  
  rm(drop_idxs)
  
  # At this point, fasta_all and metadata_all contains the needed data.
  # Now do stratified random sampling.
  set.seed(seed)
  
  # Append rowname column for fasta_all subsetting.
  # Drop this column before exporting.
  meta_grouped <- metadata_all %>%
    dplyr::group_by(variant) %>%
    tibble::rownames_to_column()
  
  # Do not preserve grouping structure (below only) to avoid NULL groups
  # If number of samples in variant group < strat_size, then filter from
  # meta_grouped and put temporarily in dropped_variants.
  # If number of samples is variant group >= strat_size, then get those
  # and place in meta_grouped, then randomly sample each of those groups
  dropped_variants <- filter(meta_grouped, n() < strat_size)
  meta_grouped <- filter(meta_grouped, n() >= strat_size)
  # TODO: consider sample_frac()
  if (nrow(meta_grouped) >= strat_size)
    meta_grouped <- sample_n(meta_grouped, strat_size)
  metadata_all <- bind_rows(meta_grouped, dropped_variants)
  
  # Remove grouping information from tibble, let downstream handle it
  metadata_all <- dplyr::ungroup(metadata_all)
  
  rm(dropped_variants)
  rm(meta_grouped)
  
  set.seed(NULL)
  
  idxs <- as.integer(metadata_all$rowname)
  fasta_all <- fasta_all[idxs]
  
  # Drop explicit rowname column, already used to subset fasta_all.
  metadata_all <- metadata_all %>% select(!rowname)
  
  # Drop rows with NA values and type mismatches.
  # Get the idxs of the dropped metadata_all rows then drop them in fasta_all.
  drop_idxs1 <- which(is.na(metadata_all), arr.ind=TRUE)[,1]
  drop_idxs2 <- c(which(is.numeric(metadata_all$sex)),
                  which(!(metadata_all$sex %vin% list("Male", "Female"))))
  drop_idxs3 <- which(lengths(fasta_all) == 0)
  drop_idxs <- unique(c(drop_idxs1, drop_idxs2, drop_idxs3))
  
  # Dropping below is analogous to select inverse.
  # pmatch creates matches, val for match and NA for no match.
  # We only take those without matches, i.e. those that won't be dropped.
  fasta_all <- fasta_all[is.na(pmatch(1:length(fasta_all), drop_idxs))]
  metadata_all <- metadata_all[is.na(pmatch(1:nrow(metadata_all), drop_idxs)),]
  
  rm(idxs, drop_idxs1, drop_idxs2, drop_idxs3, drop_idxs)
  
  # Addon: Add age_group, adjacent to age column
  metadata_all <- metadata_all %>%
    dplyr::mutate(age_group = cut(age, breaks=c(0,14,24,64,500),
                                  include.lowest=T,
                                  labels=c("0-14", "15-24", "25-64", "65+")),
                  .after = age)
  
  # At this point, data has been stratified and randomly sampled.
  # We may now return it for further cleaning and downstream use.
  message(paste("\nNumber of randomly selected samples in stratified data:",
                nrow(metadata_all)))
  message("Variant distribution in selected samples:")
  metadata_all %>%
    dplyr::group_by(variant) %>%
    dplyr::count() %>% print()
  
  list(fasta_all, metadata_all)
}
```

```{r}
list[fasta_all, metadata_all] <- get_sample(gisaid_data_path,
                                            gisaid_extract_path,
                                            seed, strat_size,
                                            country_exposure)
```

**Expanded call:**

# Step 1.5A: sanitize_sample()
metadata_all <- sanitize_sample(metadata_all)

# Step 1.5B: generate_interm()
# Note that at strat_size > nrow(Omicron), you'll be writing around 700MB
# of fasta_all_stamp.csv, so be cautious of generate_interm's space usage.
if (write_fastacsv)
  generate_interm(fasta_all, metadata_all, interm_write_path, stamp)

# Step 1.5C: compile_overview()
# compile_overview drops the submitting_lab and authors column
# after compilation, hence the reassignment to metadata_all.
metadata_all <- compile_overview(metadata_all, compile_write_path, stamp)

# Step 1.5D: make_treemaps()
# NOTE: The treemap() function in helper.R
# can generate any treemap you can think of, yeah!
#make_treemaps(metadata_all, treemaps_write_path, stamp)

# Step 2: get_kmers()
# get_kmers() prefers fasta_all in the DNAbin format
for (k in kmer_list) {
  get_kmers(fasta_all, metadata_all, k, stamp)
}

# GET KMERS FROM PRE-WRITTEN FILES (depends on strat_size)
# kmers is list of kmer dataframes
kmers <- list()
for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  k_path <- sprintf("%s/kmer_%d_%d.csv", kmers_data_path, k, strat_size)
  message(sprintf("Reading %s for later... ", k_path), appendLF = FALSE)
  kmers[[i]] <- utils::read.csv(k_path)
  message("DONE!")
}

# Step 2.5: generate_heatmap()
# for (i in 1:length(kmer_list)) {
#   k <- kmer_list[i]
#   generate_heatmap(kmers[[i]], heatmaps_write_path, k)
# }

# Step 3: dim_reduce()
The idea is that the clusters are already there. Dimensionality reduction merely allows us to see them.

for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  dim_reduce(k, kmers[[i]], dimreduce_write_path,
             tsne_seed = seed, tsne_perplexity,
             tsne_max_iter, tsne_initial_dims,
             umap_seed = seed, umap_n_neighbors,
             umap_metric, umap_min_dist, color = color, shape = shape,
#             filter1_factor = factor1, filter1_values = values1, # OPTIONAL
#             filter2_factor = factor2, filter2_values = values2, # OPTIONAL
             include_plots = include_plots)
}

#Step 4: AGNES Clustering by Variant
for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  dendrogram_create_variant(k, kmers[[i]], agnes_write_path, include_plots)
}

#Step 5: AGNES Clustering by Region
for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  dendrogram_create_region(k, kmers[[i]], agnes_write_path, include_plots)
}

message("All operations completed successfully!")

# CLEAN UP #################################################

# Clear environment
rm(list = ls()) 

# Clear packages (unloading them before another adds another compat check)
p_unload(all)  # Remove all add-ons

# Clear plots but only if there IS a plot
while (!is.null(dev.list())) dev.off()

# Clear console
# cat("\014")  # ctrl+L

# Clear mind :)