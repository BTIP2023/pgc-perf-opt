---
title: "pgc-perf-opt Pipeline"
subtitle: "A SARS-CoV-2 Variant Discriminator using Unsupervised Machine Learning (UML) Techniques"
author: "<b>Project Lead:</b> Mark Joseph Garrovillas, RCh</br><b>Interns:</b> Brylle Joshua Logro√±o, John Matthew Felices, Yenzy Urson Hebron"
institution: "Philippine Genome Center - Core Facility for Bioinformatics"
output: html_notebook
---

***

This notebook tackles the main code in `code/pipeline.R`, guiding us through the workflow's setup and execution, along with a brief exploration of the results. Code chunks that are not present or referred to in the main code, such as the setup chunk, are excluded from previews and knits.

This workflow can produce rich and intuitive clusterings of SARS-CoV-2 genome samples by treating $k$-mer count data with the following dimensionality reduction and clustering techniques:

* Principal components analysis (PCA)
* t-distributed stochastic neighbor embedding (t-SNE)
* Uniform manifold approximation (UMAP), and
* Agglomerative clustering (AGNES).

The efficiency and quality of the clusterings are also assessed, particularly the insights that can be extracted from the resulting visualizations. The $k$-mer counts of each sample are also augmented with metadata, which while not used to *perform* the clusterings to preserve the UML approach, enhances the results with descriptive labels.

The workflow is well-tested on odd $k$-mer count matrices where $k = \{3, 5, 7\}$. It is currently set to only use Philippine SARS-CoV-2 data (see `data/GISAID`), filtering out samples not collected in the Philippines (see `country_exposure` parameter), but it can be set to use other countries in the parameters section of the Setup.

***

# Primer: Data Discussion

***

# 1. Pipeline Setup

```{r setup, echo=FALSE}
# Executing this setup chunk will set the working directory to that of the
# project repository root and will add "Copy to Clipboard" buttons in the code
# chunks via `klippy`.
knitr::opts_knit$set(root.dir = normalizePath(".."))
if (!require("klippy"))
  devtools::install_github("rlesur/klippy", upgrade = FALSE)
library(klippy)
```

## A. Load Packages

If this project is not running in a project-specific environment (e.g. the custom-made pgc-perf-opt Docker image), then the following packages will also have to be installed for the pipeline to work. Else, this section will only deal with loading the dependencies.

We use the CRAN repository as our main source.

```{r message=FALSE}
options(repos = "https://cloud.r-project.org/")
```

We use the `pacman` package to conveniently organize our project's packages.

```{r message=FALSE}
if (!require("pacman"))
  install.packages("pacman")
library(pacman)
```

If we are not running on a tailor-made environment, we also install `xml2` in advance to prepare for Tidyverse installation in Linux. Note that in Windows RStudio, this is installed by default. This is also a good time to demonstrate how our development life cycle took care to maintain compatibility between Linux and Windows, and this is most evident in the pipeline and in `code/R/helper.R::write_to_log`.

```{r message=FALSE}
if (pacman::p_detectOS() == "Linux" && !pacman::p_exists(xml2, local = TRUE)) {
  install.packages("xml2", dependencies = TRUE, INSTALL_opts = c("--no-lock"))
  pacman::p_load(xml2)
}
```

Let us use `pacman` to load add-on packages as desired. We omit redundant dependencies such as `dplyr` and `ggplot2` which are already in the `tidyverse`.

```{r message=FALSE}
pacman::p_load(plyr, GGally, ggthemes, ggvis, plotly, psych,
               htmlwidgets, rio, markdown, shiny, tidyverse,
               ape, seqinr, kmer, validate, gsubfn,
               Rtsne, tsne, umap, factoextra, scales,
               RColorBrewer, ggfortify, devtools,
               ggdendro, dendextend, cluster, colorspace,
               microbenchmark, data.table, highcharter)

# ggbiplot is installed separately via install_github
if (!require(ggbiplot))
  devtools::install_github("vqv/ggbiplot", upgrade = FALSE, quiet = TRUE)
pacman::p_load(ggbiplot)
```

Remarks on some of the dependencies:

* `ape`: for reading and writing FASTA files in `DNAbin` format.
* `seqinr`: an `ape` alternative, reads and writes FASTA files in `char` format.
* `kmer`: for generating $k$-mer counts, works more efficiently with `DNAbin`.
* `validate`: for the `%vin%` membership operator.
* `gsubfn`: for destructuring more than one return value.
* `Rtsne`, `tsne`, `umap`, `ggdendro`, and `dendextend`: for accessing t-SNE, UMAP, and AGNES algorithms.
* `devtools`: supports `install_github` for installing `ggbiplot`.

## B. Load Sources

Let us fetch functions from the files in `code/R`. This requires that this notebook is loaded using the pgc-perf-opt `.Rproject` file at the root of the repository.

```{r}
source("code/R/helper.R")
source("code/R/preprocess.R")
source("code/R/kmer-analysis.R")
source("code/R/dim-reduce.R")
source("code/R/clustering.R")
```

## C. Set Parameters

Here, one may set parameters that will control the flow and behavior of the pipeline.

### Pipeline General Parameters {.tabset}

These general parameters are shared by multiple routines.

#### Code

```{r}
seed <- 1234
stamp <- get_time()
write_fastacsv <- TRUE
kmer_list <- c(7)
strat_size <- 100
include_plots <- TRUE
```

#### Description

* `seed` (`int`): The value used to reproduce the randomization of stochastic routines like `sample_n` and `tsne`. Defaults to `1234`.
* `stamp` (`str|NULL`): The suffix added to files generated by the preprocess subroutines.
  + These generated files can be found in `data/interm`, `data/overview`, and `data/kmers`.
  + Any stamp is valid, but the recommended stamps are "`str(strat_size)`" (same string as `strat_size` value, to be discussed later), "`get_time()`", and either `NULL` or "" (empty string) for no stamp.
* `write_fastacsv` (`bool`): Controls writing of intermediate files.
* `kmer_list` (`c(int)`): List of $k$ values to evaluate.
  + Default in `pipeline.R` is `c(3, 5, 7)` but not in this notebook, where only $k=7$ will be the focus of evaluations.
* `strat_size` (`int`): Number of samples randomly selected per stratum.
  + Current `nrow(data) = 24761`, hence `strat_size <- 25000` will ensure the selection of ALL samples in the entire data.
  + Valid `strat_size` will only be those with corresponding files in both `data/interm` and `data/kmers`, e.g. if there are `data/interm/`.
  + This parameter only serves to limit the amount of samples to process for low-performance computers.
* `include_plots` (`bool`): Whether or not to generate plots.
  + Used by `dim-reduce.R` and `clustering.R` (AGNES) routines.

### preprocess.R Parameters {.tabset}
These parameters control how the raw GISAID data is processed from extraction to preparing the FASTA and metadata files for downstream analysis.

* Recall that due to GISAID's terms of use, GISAID data must be manually added to `gisaid_data_path` which is most likely empty in a freshly cloned pgc-perf-opt project.
* We adopt the following convention to path I/O:
  + If a path variable is suffixed with `write_path`, the path and its children are *write-only*.
  + If a path variable is suffixed with `data_path`, the path and its children are *read-or-write*.

#### Code

```{r}
gisaid_data_path <- "data/GISAID"
gisaid_extract_path <- "data/GISAID/datasets"
country_exposure <- "Philippines"
interm_write_path <- "data/interm"
compile_write_path <- "data/overview"
```

#### Description
* `gisaid_data_path`: Where to read raw GISAID data from, assuming this contains GISAID tar files.
* `gisaid_extract_path`: Where to extract raw GISAID data to.
* `country_exposure`: Which country to filter for in `preprocess.R::get_sample()`.
* `interm_write_path`: Where to write intermediate data, e.g. fasta_all_x.fasta and metadata_all_x.csv with various suffixes.
* `compile_write_path`: Where to write data overviews to such as authors and submitting labs along with their submission counts.
  + Includes treemaps and heatmaps with various metadata hiearchies.

### kmer-analysis.R Parameters
`kmer-analysis.R` routines will always take FASTA and metadata passed from the global environment, hence running `preprocess.R` routines to completion at least once is necessary before running `kmer-analysis.R` routines. `kmer-analysis.R` will also always write its output to `kmers_write_path`.
```{r}
# Where to write (then read) kmers data from.
# Shared by dim-reduce.R routines.
kmers_data_path <- "data/kmers"
```

### dim-reduce.R Parameters {.tabset}
These parameters control the dimensionality reduction algorithms. Hypertuning these parameters may be needed to achieve good and efficient clustering. Please consult `code/R/dim-reduce.Rmd` for more in-depth explanation on `dim-reduce.R`. Also see `?tsne::tsne` and `?umap::umap` for more information.

**Important**: As we proceed, the importance. One may skip ahead to the `metadata` discussion that

#### Code

```{r}
# dim-reduce.R::dim_reduce() main parameters
dr_write_path <- "results/dim-reduce/R"
tsne_perplexity <- 40
tsne_max_iter <- 1000
tsne_initial_dims <- 50
umap_n_neighbors <- 15
umap_metric <- "euclidean"
umap_min_dist <- 0.1
dr_color <- "variant"
dr_shape <- "year"

# dim-reduce.R::dim_reduce()::pre_reduce auxiliary parameters
dr_factor1 <- NULL
dr_values1 <- NULL
dr_factor2 <- NULL
dr_values2 <- NULL
```

#### Description

Main Parameters: These are primarily filters that control the hyperparameters of the dimensionality reduction algorithms.

* `dr_write_path` (`str`): Where to write results of dimensionality reduction.
* `tsne_perplexity` (`int`): Guess about the number of close neighbors each point has.
* `tsne_max_iter` (`int`): Bounds the number of `tsne` iterations
  + Defaults to 1000, but may be increased to get closer to actual onvergence.
* `tsne_initial_dims` (`int`): Number of dimensions to "start" from, more aggressively collapsing higher dimensions.
* `umap_n_neighbors` (`int`): Constrains the size of the local neighborhood where the manifold structure of the data may be learned.
* `umap_metric` (`str`): Determines how distances between data points are computed.
  + Available metrics are: euclidean, manhattan, cosine, pearson, and pearson2.
* `umap_min_dist` (`int`): Determines how close points appear in the final layout.
* `dr_color`: Determines which metadata attribute to use for assigning colors to the clusters.
  + See [metadata columns](#metadata) for available values.
* `dr_shape`: Determines which metadata attribute to use for assigning shapes to the clusters.
  + See [metadata columns](#metadata) for available values.
  
Note that `dr_color` and `dr_shape` work together to create two-level clustering for the dimensionality reduction visualizations.

Auxiliary Parameters: These are optional parameters (can be set to `NULL`) for applying custom filters to the $k$-mer + metadata data frame, specifically `kmers[[i]]`.

* `dr_factor1` (`NULL`|`str`): Level 1 custom filter. Defaults to `NULL`.
  + See [metadata columns](#metadata) for available values.
* `dr_values1` (`NULL`|`c(any)`): List of values to filter. Defaults to `NULL`.
* `dr_factor2` (`NULL`|`str`): Level 2 custom filter. Defaults to `NULL`.
* `dr_values2` (`NULL`|`c(any)`):

Sample usage of auxiliary parameters:

### clustering.R Parameters
```{r}
agnes_write_path <- "results/dendrogram"
```

***

# 2. Run Pipeline
Instead of functions sourced externally (as we did in the setup), they are expanded here for better understanding and coherence. We still show at the top how the routine call originally looked like in the pipeline.

## A. get_sample()
This function is responsible for 

**Sourced call:**
```{r}
get_sample <- function(gisaid_data_path = "data/GISAID",
                       gisaid_extract_path = "data/GISAID/datasets",
                       seed = 1234, strat_size = 100,
                       country_exposure = "Philippines") {
  # Extract GISAID data.
  if (dir.exists(gisaid_extract_path)) {
    message("GISAID data already extracted from tar archives.")
  } else {
    message("Extracting GISAID data to data/GISAID/datasets/...")
    tars <- list.files(gisaid_data_path, pattern = ".+\\.tar")
    for (file_name in tars) {
      subdir <- str_match(file_name, pattern = "[^-]+-[^-]+")
      message(paste("Extracting to:", subdir))
      untar(paste(gisaid_data_path, file_name, sep = "/"),
            exdir = paste(gisaid_extract_path, subdir, sep = "/"))
    }
  }
  
  # Merge all extracted fasta and tsv files.
  ## fasta contains sequence, while tsv contains metadata.
  omicron_sub = c("ba275", "xbb", "xbb_1.5", "xbb_1.16", "xbb1.91")
  
  fastas <- list.files(gisaid_extract_path, recursive = TRUE,
                       pattern = ".+\\.fasta")
  tsvs <- list.files(gisaid_extract_path, recursive = TRUE,
                     pattern = ".+\\.tsv")
  nfiles <- length(fastas)
  
  ## Accumulators: fasta_all and metadata_all
  fasta_all <- list()
  metadata_all <- tibble()

  ## Warnings suppressed for data parsing, but handled cleanly so don't worry.
  suppressWarnings({
    for (i in 1:nfiles) {
      fasta_path <- paste(gisaid_extract_path, fastas[i], sep = "/")
      tsv_path <- paste(gisaid_extract_path, tsvs[i], sep = "/")
      variant <- str_match(fasta_path, pattern = "(?<=-).*(?=\\/)")
      if (variant %vin% omicron_sub) {
        variant <- "Omicron Sub"
      }
      variant <- str_to_title(variant)
      
      message(paste0("Reading ", fasta_path, "... "))
      # Parse then merge fasta file with accumulator.
      fasta <- ape::read.FASTA(fasta_path)
      fasta_all <- append(fasta_all, fasta)
      message("\bDONE.")
      
      message(paste0("Reading ", tsv_path, "... "))
      # Parse then merge metaData file with accumulator.
      # Defer sanitation after random sampling so fasta and metaData kept 1:1.
      # Can't directly col_types = "c_c_D____ccc___if_c__cc_____" because
      # of dirt in some columns, still need to use characters.
      metaData <- readr::read_tsv(tsv_path,
                                  col_select = c(1,3,5,10,11,12,16,17,19,22,23),
                                  show_col_types = FALSE)
      message("\bDONE.")
      
      # Not removing raw date as I believe it is useful for sorting or can be
      # parsed on an as-needed basis. Dropped year, month, day: just extract
      # them from the date using lubridate::{year,month,day}(date).
      metaData <- metaData %>%
        dplyr::mutate(variant = as.character(variant))
      
      # Note: Cannot use tidyr::nest(fasta or tibble(fasta)), see reason below.
      
      # Coerce guessed column types to correct types (also for bind_rows).
      # NAs introduced by coercion will be dropped later because dropping
      # metadata rows must be consistent with dropping fasta entries for
      # the reason that nested DNAbin lists are not supported in R.
      metaData$age <- as.integer(metaData$age)
      metaData$sex <- as.character(metaData$sex)
      
      metadata_all <- bind_rows(metadata_all, metaData)
    }
  })

  rm(fasta)
  rm(metaData)

  # Print out number of samples beforehand to guide future strat_size.
  message(paste("\nTotal number of samples in complete, unpruned data:",
              nrow(metadata_all)))
  message("Variant distribution in complete data:")
  metadata_all %>%
    dplyr::group_by(variant) %>%
    dplyr::count() %>% print()

  # Addon: Filter by country_exposure.
  drop_idxs <- which(metadata_all$country_exposure != country_exposure)
  fasta_all <- fasta_all[is.na(pmatch(1:length(fasta_all), drop_idxs))]
  metadata_all <- metadata_all[is.na(pmatch(1:nrow(metadata_all), drop_idxs)),]
  
  rm(drop_idxs)
  
  # At this point, fasta_all and metadata_all contains the needed data.
  # Now do stratified random sampling.
  set.seed(seed)
  
  # Append rowname column for fasta_all subsetting.
  # Drop this column before exporting.
  meta_grouped <- metadata_all %>%
    dplyr::group_by(variant) %>%
    tibble::rownames_to_column()
  
  # Do not preserve grouping structure (below only) to avoid NULL groups
  # If number of samples in variant group < strat_size, then filter from
  # meta_grouped and put temporarily in dropped_variants.
  # If number of samples is variant group >= strat_size, then get those
  # and place in meta_grouped, then randomly sample each of those groups
  dropped_variants <- filter(meta_grouped, n() < strat_size)
  meta_grouped <- filter(meta_grouped, n() >= strat_size)
  # TODO: consider sample_frac()
  if (nrow(meta_grouped) >= strat_size)
    meta_grouped <- sample_n(meta_grouped, strat_size)
  metadata_all <- bind_rows(meta_grouped, dropped_variants)
  
  # Remove grouping information from tibble, let downstream handle it
  metadata_all <- dplyr::ungroup(metadata_all)
  
  rm(dropped_variants)
  rm(meta_grouped)
  
  set.seed(NULL)
  
  idxs <- as.integer(metadata_all$rowname)
  fasta_all <- fasta_all[idxs]
  
  # Drop explicit rowname column, already used to subset fasta_all.
  metadata_all <- metadata_all %>% select(!rowname)
  
  # Drop rows with NA values and type mismatches.
  # Get the idxs of the dropped metadata_all rows then drop them in fasta_all.
  drop_idxs1 <- which(is.na(metadata_all), arr.ind=TRUE)[,1]
  drop_idxs2 <- c(which(is.numeric(metadata_all$sex)),
                  which(!(metadata_all$sex %vin% list("Male", "Female"))))
  drop_idxs3 <- which(lengths(fasta_all) == 0)
  drop_idxs <- unique(c(drop_idxs1, drop_idxs2, drop_idxs3))
  
  # Dropping below is analogous to select inverse.
  # pmatch creates matches, val for match and NA for no match.
  # We only take those without matches, i.e. those that won't be dropped.
  fasta_all <- fasta_all[is.na(pmatch(1:length(fasta_all), drop_idxs))]
  metadata_all <- metadata_all[is.na(pmatch(1:nrow(metadata_all), drop_idxs)),]
  
  rm(idxs, drop_idxs1, drop_idxs2, drop_idxs3, drop_idxs)
  
  # Addon: Add age_group, adjacent to age column
  metadata_all <- metadata_all %>%
    dplyr::mutate(age_group = cut(age, breaks=c(0,14,24,64,500),
                                  include.lowest=T,
                                  labels=c("0-14", "15-24", "25-64", "65+")),
                  .after = age)
  
  # At this point, data has been stratified and randomly sampled.
  # We may now return it for further cleaning and downstream use.
  message(paste("\nNumber of randomly selected samples in stratified data:",
                nrow(metadata_all)))
  message("Variant distribution in selected samples:")
  metadata_all %>%
    dplyr::group_by(variant) %>%
    dplyr::count() %>% print()
  
  list(fasta_all, metadata_all)
}
```

```{r}
list[fasta_all, metadata_all] <- get_sample(gisaid_data_path,
                                            gisaid_extract_path,
                                            seed, strat_size,
                                            country_exposure)
```

**Expanded call:**

# Step 1.5A: sanitize_sample()
metadata_all <- sanitize_sample(metadata_all)

## Insert remarks on fasta and metadata format (list and table) {#metadata}

# Step 1.5B: generate_interm()
# Note that at strat_size > nrow(Omicron), you'll be writing around 700MB
# of fasta_all_stamp.csv, so be cautious of generate_interm's space usage.
if (write_fastacsv)
  generate_interm(fasta_all, metadata_all, interm_write_path, stamp)

# Step 1.5C: compile_overview()
# compile_overview drops the submitting_lab and authors column
# after compilation, hence the reassignment to metadata_all.
metadata_all <- compile_overview(metadata_all, compile_write_path, stamp)

# Step 1.5D: make_treemaps()
# NOTE: The treemap() function in helper.R
# can generate any treemap you can think of, yeah!
#make_treemaps(metadata_all, treemaps_write_path, stamp)

# Step 2: get_kmers()
# get_kmers() prefers fasta_all in the DNAbin format
for (k in kmer_list) {
  get_kmers(fasta_all, metadata_all, k, stamp)
}

# GET KMERS FROM PRE-WRITTEN FILES (depends on strat_size)
# kmers is list of kmer dataframes
kmers <- list()
for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  k_path <- sprintf("%s/kmer_%d_%d.csv", kmers_data_path, k, strat_size)
  message(sprintf("Reading %s for later... ", k_path), appendLF = FALSE)
  kmers[[i]] <- utils::read.csv(k_path)
  message("DONE!")
}
 
## Insert remark on kmers data format (table)

# Step 2.5: generate_heatmap()
# for (i in 1:length(kmer_list)) {
#   k <- kmer_list[i]
#   generate_heatmap(kmers[[i]], heatmaps_write_path, k)
# }

# Step 3: dim_reduce()
The idea is that the clusters are already there. Dimensionality reduction merely allows us to see them.

for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  dim_reduce(k, kmers[[i]], dimreduce_write_path,
             tsne_seed = seed, tsne_perplexity,
             tsne_max_iter, tsne_initial_dims,
             umap_seed = seed, umap_n_neighbors,
             umap_metric, umap_min_dist, color = dr_color, shape = dr_shape,
             filter1_factor = dr_factor1, filter1_values = dr_values1,
             filter2_factor = dr_factor2, filter2_values = dr_values2,
             include_plots = include_plots)
}

# Step 4: AGNES Clustering by Variant
for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  dendrogram_create_variant(k, kmers[[i]], agnes_write_path, include_plots)
}

# Step 5: AGNES Clustering by Region
for (i in 1:length(kmer_list)) {
  k <- kmer_list[i]
  dendrogram_create_region(k, kmers[[i]], agnes_write_path, include_plots)
}

message("All operations completed successfully!")

***

# 3. Clean Up
```{r}
# Clear environment
rm(list = ls()) 

# Clear packages (unloading them before another run adds another compat check)
p_unload(all)  # Remove all add-ons

# Clear plots but only if there IS a plot
while (!is.null(dev.list())) dev.off()

# Clear console
# cat("\014")  # ctrl+L

# Clear mind :)
```

***

### Auxiliary Code {#aux}
Code with outputs that cannot be hidden but necessary for rendering the document is added here.
```{r klippy, echo=FALSE, include=TRUE}
# add "Copy to Clipboard" in previews
klippy::klippy()
```